# model3_v3.py
import streamlit as st
import torch
import threading
from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer

NEW_MAX_TOKENS = 3072
# æ¨¡å‹åŠ è½½
@st.cache_resource
def load_model():
    model_path = "/home/whu/qwen3/model"
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True,
    )
    model.eval()
    if torch.cuda.is_available():
        model.half()
        torch.backends.cudnn.benchmark = True
    return model, tokenizer

model, tokenizer = load_model()

# æµå¼ç”Ÿæˆ
def stream_generate_response(prompt, enable_thinking, max_new_tokens=NEW_MAX_TOKENS):
    messages = [{"role": "user", "content": prompt}]
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
        enable_thinking=enable_thinking
    )
    inputs = tokenizer([text], return_tensors="pt").to(model.device)

    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)
    generation_kwargs = dict(
        **inputs,
        max_new_tokens=max_new_tokens,
        do_sample=False,
        streamer=streamer,
        eos_token_id=tokenizer.eos_token_id,
        pad_token_id=tokenizer.eos_token_id,
        use_cache=True,
    )

    thread = threading.Thread(target=model.generate, kwargs=generation_kwargs)
    thread.start()
    for new_text in streamer:
        yield new_text

# æŒ‰ç»“æ„æ¨¡å—åˆ†å—æå–ä»£ç 
def explain_cpp_block(code,enable_thinking):
    prompt = f"""ä»»åŠ¡: è¯·ä½ æ‰®æ¼”ä¸€ä¸ªç»éªŒä¸°å¯Œçš„Cè¯­è¨€/c++/cudaä»£ç åˆ†æå¸ˆã€‚ä½ çš„ä»»åŠ¡æ˜¯å¯¹æˆ‘æ¥ä¸‹æ¥æä¾›çš„Cä»£ç è¿›è¡Œè¯¦ç»†åˆ†æï¼Œè¯†åˆ«å…¶ä¸­çš„å…³é”®å…ƒç´ ï¼Œå¹¶å°†ä»£ç é€»è¾‘ä¸Šåˆ†å—ã€‚
å…·ä½“æ­¥éª¤:
1.ä»£ç åˆ†æ: ä»”ç»†é˜…è¯»æˆ‘æä¾›çš„ä»£ç ã€‚è¯†åˆ«å‡ºä»£ç ä¸­çš„ä¸»è¦ç»„æˆéƒ¨åˆ†ï¼Œä¾‹å¦‚ï¼š
- å¤´æ–‡ä»¶åŒ…å« (#include): åˆ—å‡ºæ‰€æœ‰åŒ…å«çš„å¤´æ–‡ä»¶åŠå…¶ä½œç”¨ï¼ˆå¦‚æœå¯ä»¥åˆ¤æ–­ï¼‰ã€‚
- å®å®šä¹‰ (#define): åˆ—å‡ºæ‰€æœ‰å®å®šä¹‰åŠå…¶ç”¨é€”ã€‚
- ç±»å‹å®šä¹‰ (typedef): å¦‚æœæœ‰ï¼Œåˆ—å‡ºè‡ªå®šä¹‰çš„ç±»å‹åŠå…¶å«ä¹‰ã€‚
- å…¨å±€å˜é‡/å¸¸é‡: è¯†åˆ«å‡ºåœ¨å‡½æ•°å¤–éƒ¨å®šä¹‰çš„å˜é‡æˆ–ä½¿ç”¨ const/static ä¿®é¥°çš„å˜é‡ã€‚
- å‡½æ•°å®šä¹‰: è¯†åˆ«å‡ºæ‰€æœ‰è‡ªå®šä¹‰å‡½æ•°ï¼ŒåŒ…æ‹¬å…¶è¿”å›ç±»å‹ã€å‚æ•°åˆ—è¡¨å’Œå‡½æ•°ä½“ã€‚
- ç»“æ„ä½“/è”åˆä½“/æšä¸¾ (struct/union/enum): è¯†åˆ«å‡ºæ‰€æœ‰è‡ªå®šä¹‰çš„æ•°æ®ç»“æ„åŠå…¶æˆå‘˜ã€‚
- ä¸»å‡½æ•° (main): ç‰¹åˆ«å…³æ³¨ç¨‹åºçš„å…¥å£ç‚¹ã€‚
- æ³¨é‡Š: æ³¨æ„ä»£ç ä¸­çš„æ³¨é‡Šï¼Œå®ƒä»¬æœ‰åŠ©äºç†è§£ä»£ç æ„å›¾ã€‚
- å…¶ä»–é€»è¾‘å—: è¯†åˆ«å‡ºä»£ç ä¸­å…¶ä»–æœ‰æ„ä¹‰çš„é€»è¾‘ç‰‡æ®µï¼Œä¾‹å¦‚ç‰¹å®šçš„ä»£ç æ®µã€å¾ªç¯ã€æ¡ä»¶åˆ¤æ–­ç­‰ã€‚
- é€»è¾‘åˆ†å—: åŸºäºç¬¬ä¸€æ­¥çš„åˆ†æï¼Œå°†ä»£ç æŒ‰ç…§å…¶é€»è¾‘åŠŸèƒ½æˆ–ç»“æ„è¿›è¡Œåˆ’åˆ†ã€‚æ¯ä¸ªåˆ†å—åº”è¯¥ä»£è¡¨ä¸€ä¸ªç›¸å¯¹ç‹¬ç«‹æˆ–åŠŸèƒ½æ˜ç¡®çš„ä»£ç å•å…ƒã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ªå‡½æ•°å¯ä»¥æ˜¯ä¸€ä¸ªåˆ†å—ï¼Œä¸€ä¸ªç»“æ„ä½“å®šä¹‰å¯ä»¥æ˜¯ä¸€ä¸ªåˆ†å—ï¼Œå¤„ç†ç‰¹å®šä»»åŠ¡çš„ä¸€æ®µä»£ç ä¹Ÿå¯ä»¥æ˜¯ä¸€ä¸ªåˆ†å—ã€‚
2.åˆ—è¡¨å‘ˆç°: å°†ä¸Šè¿°åˆ†å—ä»¥åˆ—è¡¨çš„å½¢å¼å‘ˆç°ç»™æˆ‘ã€‚åˆ—è¡¨çš„æ¯ä¸ªæ¡ç›®åº”åŒ…å«ä»¥ä¸‹ä¿¡æ¯ï¼š
3.åˆ†å—åç§°/æ ‡è¯†: ç”¨ç®€æ´æ˜äº†çš„è¯è¯­å‘½åè¿™ä¸ªåˆ†å—ï¼Œä¾‹å¦‚â€œå‡½æ•°ï¼šcalculateSumâ€ã€â€œç»“æ„ä½“å®šä¹‰ï¼šPersonâ€ã€â€œä¸»å‡½æ•°å…¥å£â€ã€â€œå…¨å±€å˜é‡å£°æ˜â€ç­‰ã€‚
4.è§£é‡Šæ¯ä¸€ä¸ªä»£ç å—çš„åŠŸèƒ½ä¸ä½œç”¨ï¼ï¼
5.ä»£ç ä¸­ä¸å­˜åœ¨çš„å…³é”®å…ƒç´ ï¼Œç¦æ­¢è¾“å‡ºï¼ï¼
è¾“å…¥:
{code}
"""
    return stream_generate_response(prompt,enable_thinking)

def analyze_performance(code):
    prompt = f"""è¯·ä»ä¸‹é¢è¿™æ®µä»£ç ä¸­åˆ†æå…¶æ€§èƒ½é—®é¢˜ï¼ŒåŒ…æ‹¬ï¼š
- æ—¶é—´å¤æ‚åº¦å’Œç©ºé—´å¤æ‚åº¦
- æ˜¯å¦æœ‰ä¸å¿…è¦çš„å†…å­˜ç”³è¯·æˆ–é‡Šæ”¾ï¼šå †ã€æ ˆã€å¸¸é‡åŒºã€å¯„å­˜å™¨ç­‰
- æ˜¯å¦å­˜åœ¨å¾ªç¯ä¼˜åŒ–ç©ºé—´
- STL ä½¿ç”¨æ˜¯å¦åˆç†
- æŒ‡é’ˆ/æ™ºèƒ½æŒ‡é’ˆä½¿ç”¨åŠå…¶ä»£ç å®ç°
- å¤šçº¿ç¨‹æˆ–å¼‚æ­¥æ½œåŠ›
- æµ‹è¯•ç¨‹åºæ€§èƒ½åŠå…¶ä»£ç 
- æ€»ä½“æ€§èƒ½ç“¶é¢ˆç‚¹
```cpp\n{code}\n```"""
    return stream_generate_response(prompt,enable_thinking)

# ä¼˜åŒ–å»ºè®®
def analyze_optimization(code):
    prompt = f"""è¯·æ€»ç»“ä»¥ä¸‹ä»£ç çš„åŠŸèƒ½ä¸ç»“æ„ï¼Œå¹¶æå‡ºå¯è¡Œçš„ä¼˜åŒ–å»ºè®®ï¼š
```cpp\n{code}\n```"""
    return stream_generate_response(prompt,enable_thinking)

# UI æ„å»º
st.title("ç¨‹åºåˆ†æä¸ä¼˜åŒ–")

st.set_page_config(page_title="ä»£ç ç»“æ„åˆ†å—ä¸è§£é‡Š", layout="wide")
code = st.text_area("è¯·è¾“å…¥ä»£ç ï¼š", height=300)

mode = st.radio("é€‰æ‹©åˆ†ææ¨¡å¼ï¼š", ["âš¡ å¿«é€Ÿåˆ†æ", "ğŸŒŠ æ·±åº¦åˆ†æ"])
enable_thinking = mode.startswith("ğŸŒŠ")

if st.button("ğŸš€ å¼€å§‹åˆ†å—åˆ†æ"):
    if not code.strip():
        st.warning("è¯·è¾“å…¥æœ‰æ•ˆçš„ C++ ä»£ç ")
    else:
        st.info("ğŸ“¦ æ­£åœ¨åˆ†å—æå–ç»“æ„å¹¶åˆ†ææ¨¡å—...")
        st.markdown("## ğŸ” åˆ†å—è§£é‡Š")
        with st.expander("ğŸ’¬ ç‚¹å‡»å±•å¼€è§£é‡Š", expanded=True):
            explanation = ""
            explain_area = st.empty()
            for chunk in explain_cpp_block(code, enable_thinking):
                explanation += chunk
                explain_area.markdown(explanation)
        st.markdown("---")

        st.markdown("## ğŸ“ˆ æ€§èƒ½åˆ†æ")
        perf_result = ""
        perf_area = st.empty()
        for chunk in analyze_performance(code):
            perf_result += chunk
            perf_area.markdown(perf_result)

        st.markdown("## ğŸ›  ä¼˜åŒ–å»ºè®®")
        opt_result = ""
        opt_area = st.empty()
        for chunk in analyze_optimization(code):
            opt_result += chunk
            opt_area.markdown(opt_result)

st.markdown("---")
st.markdown("""
### ğŸ”§ åŠŸèƒ½è¯´æ˜
- âœ… è‡ªåŠ¨å°† C++ ä»£ç æŒ‰ç»“æ„åˆ’åˆ†ä¸ºå¤šä¸ªæ¨¡å—ï¼ˆç±»/å‡½æ•°/å˜é‡ï¼‰
- âœ… æ¯å—ç»“æ„è‡ªåŠ¨è¯†åˆ«å¹¶ç”Ÿæˆè§£é‡Š
- âœ… æµå¼é€å¥è¾“å‡ºè§£é‡Šå†…å®¹
- âœ… ä¸ä½¿ç”¨ JSONï¼Œä¿æŒè‡ªç„¶ä»£ç åˆ†å—é£æ ¼
""")

