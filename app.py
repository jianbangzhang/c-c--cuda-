# model3_v3.py
import streamlit as st
import torch
import threading
import datetime
from concurrent.futures import ThreadPoolExecutor
from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer

NEW_MAX_TOKENS = 3072

# ------------------------- æ¨¡å‹åŠ è½½ï¼ˆä¼˜åŒ–ååˆ†ç¦»ï¼‰ -------------------------
@st.cache_resource
def load_tokenizer():
    model_path = "/home/whu/qwen3/model"
    return AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)

@st.cache_resource
def load_model():
    model_path = "/home/whu/qwen3/model"
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True,
    )
    model.eval()
    if torch.cuda.is_available():
        model.half()
        torch.backends.cudnn.benchmark = True
    return model

tokenizer = load_tokenizer()
model = load_model()

# ------------------------- é€šç”¨æµå¼ç”Ÿæˆ -------------------------
def stream_generate_response(prompt, enable_thinking, max_new_tokens=NEW_MAX_TOKENS):
    messages = [{"role": "user", "content": prompt}]
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
        enable_thinking=enable_thinking
    )
    inputs = tokenizer([text], return_tensors="pt").to(model.device)

    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)
    generation_kwargs = dict(
        **inputs,
        max_new_tokens=max_new_tokens,
        do_sample=False,
        streamer=streamer,
        eos_token_id=tokenizer.eos_token_id,
        pad_token_id=tokenizer.eos_token_id,
        use_cache=True,
    )

    thread = threading.Thread(target=model.generate, kwargs=generation_kwargs)
    thread.start()
    for new_text in streamer:
        yield new_text

def run_stream_analysis(func, code, enable_thinking):
    result = ""
    for chunk in func(code, enable_thinking):
        result += chunk
    return result

# ------------------------- ä¸‰ç§åˆ†ææ–¹æ³• -------------------------
def explain_cpp_block(code,enable_thinking):
    prompt = f"""ä»»åŠ¡: è¯·ä½ æ‰®æ¼”ä¸€ä¸ªç»éªŒä¸°å¯Œçš„Cè¯­è¨€/c++/cudaä»£ç åˆ†æå¸ˆã€‚ä½ çš„ä»»åŠ¡æ˜¯å¯¹æˆ‘æ¥ä¸‹æ¥æä¾›çš„Cä»£ç è¿›è¡Œè¯¦ç»†åˆ†æï¼Œè¯†åˆ«å…¶ä¸­çš„å…³é”®å…ƒç´ ï¼Œå¹¶å°†ä»£ç é€»è¾‘ä¸Šåˆ†å—ã€‚
å…·ä½“æ­¥éª¤:
1.ä»£ç åˆ†æ: ä»”ç»†é˜…è¯»æˆ‘æä¾›çš„ä»£ç ã€‚è¯†åˆ«å‡ºä»£ç ä¸­çš„ä¸»è¦ç»„æˆéƒ¨åˆ†ï¼Œä¾‹å¦‚ï¼š
- å¤´æ–‡ä»¶åŒ…å« (#include): åˆ—å‡ºæ‰€æœ‰åŒ…å«çš„å¤´æ–‡ä»¶åŠå…¶ä½œç”¨ï¼ˆå¦‚æœå¯ä»¥åˆ¤æ–­ï¼‰ã€‚
- å®å®šä¹‰ (#define): åˆ—å‡ºæ‰€æœ‰å®å®šä¹‰åŠå…¶ç”¨é€”ã€‚
- ç±»å‹å®šä¹‰ (typedef): å¦‚æœæœ‰ï¼Œåˆ—å‡ºè‡ªå®šä¹‰çš„ç±»å‹åŠå…¶å«ä¹‰ã€‚
- å…¨å±€å˜é‡/å¸¸é‡: è¯†åˆ«å‡ºåœ¨å‡½æ•°å¤–éƒ¨å®šä¹‰çš„å˜é‡æˆ–ä½¿ç”¨ const/static ä¿®é¥°çš„å˜é‡ã€‚
- å‡½æ•°å®šä¹‰: è¯†åˆ«å‡ºæ‰€æœ‰è‡ªå®šä¹‰å‡½æ•°ï¼ŒåŒ…æ‹¬å…¶è¿”å›ç±»å‹ã€å‚æ•°åˆ—è¡¨å’Œå‡½æ•°ä½“ã€‚
- ç»“æ„ä½“/è”åˆä½“/æšä¸¾ (struct/union/enum): è¯†åˆ«å‡ºæ‰€æœ‰è‡ªå®šä¹‰çš„æ•°æ®ç»“æ„åŠå…¶æˆå‘˜ã€‚
- ä¸»å‡½æ•° (main): ç‰¹åˆ«å…³æ³¨ç¨‹åºçš„å…¥å£ç‚¹ã€‚
- æ³¨é‡Š: æ³¨æ„ä»£ç ä¸­çš„æ³¨é‡Šï¼Œå®ƒä»¬æœ‰åŠ©äºç†è§£ä»£ç æ„å›¾ã€‚
- å…¶ä»–é€»è¾‘å—: è¯†åˆ«å‡ºä»£ç ä¸­å…¶ä»–æœ‰æ„ä¹‰çš„é€»è¾‘ç‰‡æ®µï¼Œä¾‹å¦‚ç‰¹å®šçš„ä»£ç æ®µã€å¾ªç¯ã€æ¡ä»¶åˆ¤æ–­ç­‰ã€‚
- é€»è¾‘åˆ†å—: åŸºäºç¬¬ä¸€æ­¥çš„åˆ†æï¼Œå°†ä»£ç æŒ‰ç…§å…¶é€»è¾‘åŠŸèƒ½æˆ–ç»“æ„è¿›è¡Œåˆ’åˆ†ã€‚æ¯ä¸ªåˆ†å—åº”è¯¥ä»£è¡¨ä¸€ä¸ªç›¸å¯¹ç‹¬ç«‹æˆ–åŠŸèƒ½æ˜ç¡®çš„ä»£ç å•å…ƒã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ªå‡½æ•°å¯ä»¥æ˜¯ä¸€ä¸ªåˆ†å—ï¼Œä¸€ä¸ªç»“æ„ä½“å®šä¹‰å¯ä»¥æ˜¯ä¸€ä¸ªåˆ†å—ï¼Œå¤„ç†ç‰¹å®šä»»åŠ¡çš„ä¸€æ®µä»£ç ä¹Ÿå¯ä»¥æ˜¯ä¸€ä¸ªåˆ†å—ã€‚
2.åˆ—è¡¨å‘ˆç°: å°†ä¸Šè¿°åˆ†å—ä»¥åˆ—è¡¨çš„å½¢å¼å‘ˆç°ç»™æˆ‘ã€‚åˆ—è¡¨çš„æ¯ä¸ªæ¡ç›®åº”åŒ…å«ä»¥ä¸‹ä¿¡æ¯ï¼š
3.åˆ†å—åç§°/æ ‡è¯†: ç”¨ç®€æ´æ˜äº†çš„è¯è¯­å‘½åè¿™ä¸ªåˆ†å—ï¼Œä¾‹å¦‚â€œå‡½æ•°ï¼šcalculateSumâ€ã€â€œç»“æ„ä½“å®šä¹‰ï¼šPersonâ€ã€â€œä¸»å‡½æ•°å…¥å£â€ã€â€œå…¨å±€å˜é‡å£°æ˜â€ç­‰ã€‚
4.è§£é‡Šæ¯ä¸€ä¸ªä»£ç å—çš„åŠŸèƒ½ä¸ä½œç”¨ï¼ï¼
5.ä»£ç ä¸­ä¸å­˜åœ¨çš„å…³é”®å…ƒç´ ï¼Œç¦æ­¢è¾“å‡ºï¼ï¼
è¾“å…¥:
{code}
"""
    return stream_generate_response(prompt,enable_thinking)

def analyze_performance(code,enable_thinking):
    prompt = f"""è¯·ä»ä¸‹é¢è¿™æ®µä»£ç ä¸­åˆ†æå…¶æ€§èƒ½é—®é¢˜ï¼ŒåŒ…æ‹¬ï¼š
- æ—¶é—´å¤æ‚åº¦å’Œç©ºé—´å¤æ‚åº¦
- æ˜¯å¦æœ‰ä¸å¿…è¦çš„å†…å­˜ç”³è¯·æˆ–é‡Šæ”¾ï¼šå †ã€æ ˆã€å¸¸é‡åŒºã€å¯„å­˜å™¨ç­‰
- æ˜¯å¦å­˜åœ¨å¾ªç¯ä¼˜åŒ–ç©ºé—´
- STL ä½¿ç”¨æ˜¯å¦åˆç†
- æŒ‡é’ˆ/æ™ºèƒ½æŒ‡é’ˆä½¿ç”¨åŠå…¶ä»£ç å®ç°
- å¤šçº¿ç¨‹æˆ–å¼‚æ­¥æ½œåŠ›
- æµ‹è¯•ç¨‹åºæ€§èƒ½åŠå…¶ä»£ç 
- æ€»ä½“æ€§èƒ½ç“¶é¢ˆç‚¹
```cpp\n{code}\n```"""
    return stream_generate_response(prompt,enable_thinking)

# ä¼˜åŒ–å»ºè®®
def analyze_optimization(code,enable_thinking):
    prompt = f"""è¯·æ€»ç»“ä»¥ä¸‹ä»£ç çš„åŠŸèƒ½ä¸ç»“æ„ï¼Œå¹¶æå‡ºå¯è¡Œçš„ä¼˜åŒ–å»ºè®®ï¼š
```cpp\n{code}\n```"""
    return stream_generate_response(prompt,enable_thinking)

# UI æ„å»º
st.set_page_config(page_title="ç¨‹åºç¼–å†™ã€åˆ†æä¸ä¼˜åŒ–", layout="wide")

# é¡µé¢å¤´éƒ¨ä¿¡æ¯ï¼ˆå±…ä¸­æ ‡é¢˜ + ä½œè€… + é‚®ç®±ï¼‰
st.markdown("""
<div style="text-align: center;">
    <h1>ğŸ“˜ ç¨‹åºç¼–å†™ã€åˆ†æä¸ä¼˜åŒ–</h1>
    <p><strong>ğŸ‘¤ ä½œè€…ï¼š</strong>jianbang zhang</p>
    <p><strong>ğŸ“§ è”ç³»é‚®ç®±ï¼š</strong><a href="mailto:queju@example.com">whdx072018@foxmail.com</a></p>
</div>
""", unsafe_allow_html=True)

st.set_page_config(page_title="ä»£ç ç»“æ„åˆ†å—ä¸è§£é‡Š", layout="wide")
code = st.text_area("è¯·è¾“å…¥ä»£ç æˆ–ä»»åŠ¡ï¼š", height=300)
mode = st.radio("é€‰æ‹©åˆ†ææ¨¡å¼ï¼š", ["âš¡ å¿«é€Ÿåˆ†æ", "ğŸŒŠ æ·±åº¦åˆ†æ"])
enable_thinking = mode.startswith("ğŸŒŠ")

if st.button("ğŸ§¹ æ¸…ç†æ¨¡å‹ç¼“å­˜"):
    load_model.clear()
    load_tokenizer.clear()
    st.success("âœ… æ¨¡å‹ç¼“å­˜å·²æ¸…é™¤ã€‚ä¸‹æ¬¡åˆ†æå°†é‡æ–°åŠ è½½æ¨¡å‹ã€‚")

if st.button("ğŸš€ å¼€å§‹åˆ†å—åˆ†æ"):
    if not code.strip():
        st.warning("è¯·è¾“å…¥æœ‰æ•ˆçš„ä»£ç æˆ–ä»»åŠ¡ï¼š")
    else:
        all_results = f"# â±ï¸ åˆ†ææ—¶é—´ï¼š{datetime.datetime.now()}\n\n"
        all_results += f"## ğŸ’» è¾“å…¥ä»£ç \n```cpp\n{code}\n```\n\n"

        # âœ… ç¬¬1éƒ¨åˆ†ï¼šç»“æ„åˆ†å—åˆ†æï¼ˆæµå¼ï¼‰
        st.markdown("## ğŸ§± åˆ†å—ç»“æ„è§£é‡Š")
        with st.expander("ğŸ” ç‚¹å‡»å±•å¼€ç»“æ„åˆ†æ", expanded=True):
            explanation = ""
            explain_area = st.empty()
            for chunk in explain_cpp_block(code, enable_thinking):
                explanation += chunk
                explain_area.markdown(explanation)
            all_results += f"## ğŸ” åˆ†å—è§£é‡Š\n{explanation}\n\n"

        # âœ… ç¬¬2éƒ¨åˆ†ï¼šæ€§èƒ½åˆ†æ + ä¼˜åŒ–å»ºè®®ï¼ˆå¹¶å‘åŠ é€Ÿï¼‰
        with st.spinner("æ­£åœ¨åˆ†ææ€§èƒ½ä¸ä¼˜åŒ–å»ºè®®..."):
            with ThreadPoolExecutor() as executor:
                perf_future = executor.submit(run_stream_analysis, analyze_performance, code, enable_thinking)
                opt_future = executor.submit(run_stream_analysis, analyze_optimization, code, enable_thinking)

                perf_result = perf_future.result()
                opt_result = opt_future.result()

        st.markdown("## ğŸ“ˆ æ€§èƒ½åˆ†æ")
        with st.expander("ğŸ’¬ ç‚¹å‡»å±•å¼€æ€§èƒ½åˆ†æ", expanded=True):
            st.markdown(perf_result)
        all_results += f"## ğŸ“ˆ æ€§èƒ½åˆ†æ\n{perf_result}\n\n"

        st.markdown("## ğŸ›  ä¼˜åŒ–å»ºè®®")
        with st.expander("ğŸ’¬ ç‚¹å‡»å±•å¼€ä¼˜åŒ–å»ºè®®", expanded=True):
            st.markdown(opt_result)
        all_results += f"## ğŸ›  ä¼˜åŒ–å»ºè®®\n{opt_result}\n\n"

        # âœ… Markdown å¯¼å‡º
        filename = f"code_analysis_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        st.download_button(
            label="ğŸ’¾ ä¿å­˜åˆ†æç»“æœä¸º Markdown æ–‡ä»¶",
            data=all_results,
            file_name=filename,
            mime="text/markdown"
        )


st.markdown("---")
st.markdown("""
### ğŸ”§ åŠŸèƒ½è¯´æ˜
- âœ… è‡ªåŠ¨ç»“æ„åˆ†æ + æ€§èƒ½ç“¶é¢ˆåˆ†æ + ä¼˜åŒ–å»ºè®®
- âœ… æµå¼é€å—è¾“å‡º + ä¸€é”®ä¿å­˜ä¸º Markdown
- âœ… åˆ†ææ¨¡å¼åˆ‡æ¢ + ç¼“å­˜æ¸…ç†æŒ‰é’®
- âœ… åˆ†æè¿‡ç¨‹å¤šçº¿ç¨‹å¹¶å‘ï¼ŒåŠ é€Ÿè¿è¡Œ
""")
